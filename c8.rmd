---
output:
  html_document: default
  pdf_document: default
fontsize: 10pt
geometry: margin=0.75in
---

# Predicting the Quality of Exercise Activities

## Vin Lam

The goal of this analysis is to use data generated by accelerometers to predict the quality of how various
exercise activities were performed among 6 test subjects. A wide variety of measurements were recorded, as 
well as the manner in which the activity was performed within the training set (the "classe" column). First,
the data will be cleaned by fixing missing values and converting variables into the correct classes. Then, data
will be preprocessed using the caret package, first with PCA and then removing predictors with near zero variance. I will experiment with three different models and assess the performance of each model on the validation set before picking the best model to predict on the test set. 

First things first, we load the necessary libraries before beginning. 
```{r global options, include = FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

```{r libraries}
library(caret);library(tidyverse);library(parallel);library(doParallel)
```

Reading in the data locally, then removing unneeded columns such as username and timestamps.

```{r readdata}
train = read.csv("pml-training.csv"); train = as_tibble(train)
test = read.csv("pml-testing.csv"); test = as_tibble(test)

train = train[,-c(1:7)]; test = test[,-c(1:7)]
```

Looping through each column of the training set and converting factor variables that should be numeric. Storing
the classe variable for future use, since it is the only variable that should be a factor.

```{r convert}
classe = train$classe

train = train %>% mutate_if(is.factor, as.character)
train = train %>% mutate_if(is.character, as.numeric)
```

Now we need to check for missing values. We can easily check each column and return the sum of na values from 
each column. Then, if data is missing for that column in more than 90% of observations, that column will be 
omitted from the dataframe. Afterwards, the classe variable in its proper factor form is re-bound to the dataframe. 

```{r missing}
head(colSums(is.na(train)),10);head(colSums(is.na(test)),10)

train = train[,which(colMeans(!is.na(train)) > 0.9)]; test = test[,which(colMeans(!is.na(test)) > 0.9)]

train = cbind(train,classe); train = as_tibble(train)
```

We are left with 52 predictors after removing columns with over 90% of values missing. The next step is to split
the training set into a training and validation set to build our models with. 

```{r split}
set.seed(69420)
inTrain = createDataPartition(y = train$classe,p=0.7,list=FALSE)
training = train[inTrain,]
testing = train[-inTrain,]
```

The first model I would like to train is a simple random forest model on only the principal components of the training data. The reason for this is that the crossvalidation method I want to use is repeated k-folds cross validation. This is a very lengthy process, especially with many variables, but it boosts model performance quite significantly. Luckily, we can take advantage of dimension reduction via principal components, which will make training a random forest model much quicker. The main disadvantage with this approach is reduced accuracy, but it will provide great insight on the performance of random forest before actually investing the time to train a rf model on the entire training set. We preprocess the training and validation data and create new dataframes containing the principal components, but making sure to use the preprocessed object from the training data to form the new validation dataframe. 

```{r pca}
preObj = preProcess(training[,-53],method="pca")
trainingProc = predict(preObj,training[,-53])
testingProc = predict(preObj,testing[,-53])
```

Now, we can train our first model. I take advantage of the allowParallel parameter in the train function to use more of my PC's computing power. I specify repeated k-folds CV as my method of CV, with 10 folds and 5 repeats.
10 is chosen since it balances the bias versus variance trade-off very well.

```{r rf_pca, cache = TRUE}
cluster = makeCluster(detectCores()-1)
registerDoParallel(cluster)

tControl = trainControl(method = "repeatedcv",
                        number = 10,
                        repeats = 5,
                        allowParallel = TRUE)

model1 = train(x = trainingProc, y = training$classe, method = "rf", trControl = tControl)

pred1 = predict(model1, newdata=testingProc)

confusionMatrix(pred1, testing$classe)
```

The model does very well despite using only PCA. The accuracy is over 97%, suggesting that random forest is a good candidate for prediction and that it would be worth training a model on every variable in the dataset.
With this in mind, gradient boosting might be another good candidate taking into consideration the performance of the previous model. Both are ensemble methods using classification trees so boosting is worth trying out. I will use new preprocessed dataframes, this time eliminating near zero variance predictors from our training set.  

```{r gbm, cache = TRUE}
preObj2 = preProcess(training[,-53], method = "nzv")
trainingProc2 = predict(preObj2,training[,-53])
testingProc2 = predict(preObj2,testing[,-53])

model2 = train(x = trainingProc2, y = training$classe, method = "gbm", trControl = tControl, verbose = FALSE)

pred2 = predict(model2, newdata=testingProc2)

confusionMatrix(pred2, testing$classe)
```

While the model performs well, it is weaker than the random forest model trained on only the principal components. The final model selected is a random forest model trained on all 52 predictors. 

```{r rf, cache = TRUE}
model3 = train(x = trainingProc2, y=training$classe, method = "rf", trControl = tControl)

pred3 = predict(model3, newdata=testingProc2)

confusionMatrix(pred3, testing$classe)

stopCluster(cluster)
registerDoSEQ()
```

The final model has the best in sample accuracy out of the 3 models, boasting over 99%. The importance of each predictor to the algorithm can be seen here.

```{r varimp, message = FALSE, fig.height = 7}
plot(varImp(model3))
```

With the lower bound of the 95% CI for the accuracy of the third model being ~99%, I would expect the out-of-sample error to be around 1%. At this point, we might try to pursue marginal improvements in the model through other means such as combining predictors, hypertuning parameters, and experimenting with more powerful/computationally demanding algorithms like XGBoost. However, I don't think it is necessary as the random forest model trained on our near-zero-variance preprocessed data performs exceptionally.

Lastly, we can make predictions on the test set.

```{r test}
testpred = predict(model3, test)
testpred
```